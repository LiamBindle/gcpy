""" Core utilities for handling GEOS-Chem data """

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os

import xarray as xr
import xbpch
import numpy as np

def open_dataset(filename, **kwargs):
    """ Load and decode a dataset from an output file generated by GEOS-Chem

    This method inspects a GEOS-Chem output file and chooses a way to load it
    into memory as an xarray Dataset. Because two different libraries to
    support BPCH and netCDF outputs, you may need to pass additional keyword
    arguments to the function. See the Examples below.

    Parameters
    ----------
    filename : str
        Path to a GEOS-Chem output file (netCDF or BPCH format) which can be
        loaded through either xarray or xbpch. Note that xarray conventions for
        netCDF files apply.
    **kwargs
        Additional keyword arguments to be passed directly to
        `xarray.open_dataset` or `xbpch.open_bpchdataset`.

    Returns
    -------
    dataset : xarray.Dataset
        The dataset loaded from the referenced filename.

    See Also
    --------
    xarray.open_dataset
    xbpch.open_bpchdataset
    open_mfdataset

    Examples
    --------

    Open a legacy BPCH output file:

    >>> ds = open_dataset("my_GEOS-Chem_run.bpch",
    ...                   tracerinfo_file='tracerinfo.dat',
    ...                   diaginfo_file='diaginfo.dat')

    Open a netCDF output file, but disable metadata decoding:
    >>> ds = open_dataset("my_GCHP_data.nc",
    ...                   decode_times=False, decode_cf=False)

    """

    basename, file_extension = os.path.splitext(filename)

    if file_extension == '.bpch':
        _opener = xbpch.open_bpchdataset
    elif file_extension == '.nc':
        _opener = xr.open_dataset
    else:
        raise ValueError("Found unknown file extension ({}); please pass a "
                         "BPCH or netCDF file with extension 'bpch' or 'nc'."
                         .format(file_extension))

    return _opener(filename, **kwargs)


def open_mfdataset(filenames, concat_dim='time', compat='no_conflicts',
                   preprocess=None, lock=None, **kwargs):
    """ Load and decode multiple GEOS-Chem output files as a single Dataset.

    Parameters
    ----------
    filenames : list of strs
        Paths to GEOS-Chem output files to load. Must have the same extension
        and be able to be concatenated along some common axis.
    concat_dim : str, default='time'
        Dimension to concatenate Datasets over. We default to "time" since this
        is how GEOS-Chem splits output files.
    compat : {'identical', 'equals', 'broadcast_equals',
              'no_conflicts'}, optional
        String indicating how to compare variables of the same name for
        potential conflicts when merging:
        - 'broadcast_equals': all values must be equal when variables are
          broadcast against each other to ensure common dimensions.
        - 'equals': all values and dimensions must be the same.
        - 'identical': all values, dimensions and attributes must be the
          same.
        - 'no_conflicts': only values which are not null in both datasets
          must be equal. The returned dataset then contains the combination
          of all non-null values.
    preprocess : callable (optional)
        A pre-processing function to apply to each Dataset prior to
        concatenation
    lock : False, True, or threading.Lock (optional)
        Passed to :py:func:`dask.array.from_array`. By default, xarray
        employs a per-variable lock when reading data from NetCDF files,
        but this model has not yet been extended or implemented for bpch files
        and so this is not actually used. However, it is likely necessary
        before dask's multi-threaded backend can be used
    **kwargs
        Additional keyword arguments to be passed directly to
        `xbpch.open_mfbpchdataset` or `xarray.open_mfdataset`.

    Returns
    -------
    dataset : xarray.Dataset
        A dataset containing the data in the specified input filenames.

    See Also
    --------
    xarray.open_mfdataset
    xbpch.open_mfbpchdataset
    open_dataset

    """

    try:
        test_fn = filenames[0]
    except:
        raise ValueError("Must pass a list with at least one filename")

    basename, file_extension = os.path.splitext(test_fn)

    if file_extension == '.bpch':
        _opener = xbpch.open_mfbpchdataset
    elif file_extension == '.nc':
        _opener = xr.open_mfdataset
    else:
        raise ValueError("Found unknown file extension ({}); please pass a "
                         "BPCH or netCDF file with extension 'bpch' or 'nc'."
                         .format(file_extension))

    return _opener(filenames, concat_dim=concat_dim, compat=compat,
                   preprocess=preprocess, lock=lock, **kwargs)

def get_gcc_filepath(outputdir, collection, day, time):
    if collection == 'Emissions':
        filepath = os.path.join(outputdir, 'HEMCO_diagnostics.{}{}.nc'.format(day,time))
    else:
        filepath = os.path.join(outputdir, 'GEOSChem.{}.{}_{}z.nc4'.format(collection,day,time))
    return filepath

def check_paths( refpath, devpath):
    if not os.path.exists(refpath):
        print('ERROR! Path 1 does not exist: {}'.format(refpath))
    else:
        print('Path 1 exists: {}'.format(refpath))
    if not os.path.exists(devpath):
        print('ERROR! Path 2 does not exist: {}'.format(devpath))
    else:
        print('Path 2 exists: {}'.format(devpath))

def compare_varnames(refdata, refstr, devdata, devstr):
    
    # Find common variables in collection by generating lists and list overlap
    refvars = [k for k in refdata.data_vars.keys()]
    devvars= [k for k in devdata.data_vars.keys()]
    commonvars = sorted(list(set(refvars).intersection(set(devvars))))
    refonly = [v for v in refvars if v not in devvars]
    devonly = [v for v in devvars if v not in refvars]
    dimmismatch = [v for v in commonvars if refdata[v].ndim != devdata[v].ndim]
    commonvars1D = [v for v in commonvars if refdata[v].ndim == 2]
    commonvars2D = [v for v in commonvars if refdata[v].ndim == 3]
    commonvars3D = [v for v in commonvars if devdata[v].ndim == 4]
    
    # Print information on common and mismatching variables, as well as dimensions
    print('{} common variables: '.format(len(commonvars)))
    if len(refonly) > 0:
        print('{} variables in {} only (skip)'.format(len(refonly),refstr))
        print('   Variable names: {}'.format(refonly))
    if len(devonly) > 0:
        print('{} variables in {} only (skip)'.format(len(devonly),devstr))
        print('   Variable names: {}'.format(devonly))
    if len(dimmismatch) > 0:
        print('{} common variables have different dimensions'.format(len(dimmismatch)))
        print('   Variable names: {}'.format(dimmismatch))        

    return [commonvars, commonvars1D, commonvars2D, commonvars3D]

def compare_stats(refdata, refstr, devdata, devstr, varname):
    refvar = refdata[varname]
    devvar = devdata[varname]
    units = refdata[varname].units
    print('Data units:')
    print('    {}:  {}'.format(refstr,units))
    print('    {}:  {}'.format(devstr,units))
    print('Array sizes:')
    print('    {}:  {}'.format(refstr,refvar.shape))
    print('    {}:  {}'.format(devstr,devvar.shape))
    print('Global stats:')
    print('  Mean:')
    print('    {}:  {}'.format(refstr,np.round(refvar.values.mean(),20)))
    print('    {}:  {}'.format(devstr,np.round(devvar.values.mean(),20)))
    print('  Min:')
    print('    {}:  {}'.format(refstr,np.round(refvar.values.min(),20)))
    print('    {}:  {}'.format(devstr,np.round(devvar.values.min(),20)))
    print('  Max:')
    print('    {}:  {}'.format(refstr,np.round(refvar.values.max(),20)))
    print('    {}:  {}'.format(devstr,np.round(devvar.values.max(),20)))
    print('  Sum:')
    print('    {}:  {}'.format(refstr,np.round(refvar.values.sum(),20)))
    print('    {}:  {}'.format(devstr,np.round(devvar.values.sum(),20)))

def get_collection_data(refdir, devdir, collection, day, time):
    reffile = get_gcc_filepath(refdir, collection, day, time)
    devfile = get_gcc_filepath(devdir, collection, day, time)
    check_paths(reffile, devfile)
    refdata = xr.open_dataset(reffile)
    devdata = xr.open_dataset(devfile)
    return [refdata, devdata]

def make_grid_LL(llres):
    [dlat,dlon] = list(map(float, llres.split('x')))
    lon_b = np.linspace(-180 - dlon/2, 180 - dlon/2, int(360/dlon) + 1, endpoint=True)
    lat_b = np.linspace(-90 - dlat/2, 90 + dlat/2, int(180/dlat) + 2, endpoint=True).clip(-90,90)
    lat = (lat_b[1:] + lat_b[:-1]) / 2
    lon = (lon_b[1:] + lon_b[:-1]) / 2
    llgrid = {'lat': lat, 
              'lon': lon, 
              'lat_b': lat_b, 
              'lon_b': lon_b}
    return llgrid



